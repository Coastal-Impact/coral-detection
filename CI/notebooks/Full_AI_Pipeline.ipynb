{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PI8L2EvE9F_"
      },
      "source": [
        "### Setting up Basic Stuff\n",
        "* Import Libraries\n",
        "* Install Libraries\n",
        "* Defining Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMMeK9qdFdII"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# sam\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "# ultralytics\n",
        "!pip install ultralytics\n",
        "\n",
        "# other installations\n",
        "!pip install -q roboflow dataclasses-json supervision==0.17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH1NR70nEoZ_"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "import torch\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import supervision as sv\n",
        "import numpy as np\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys6ApBXnFD2o"
      },
      "outputs": [],
      "source": [
        "# setting up home variable\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)\n",
        "\n",
        "# setting up global variables\n",
        "POSSIBLE_IMAGE_EXTENSIONS = [\"jpg\", \"JPG\", \"png\", \"PNG\", \"JPEG\", 'jpeg']\n",
        "RESIZED_IMAGE_SIZE = (640, 490)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKUBdYDDGq6S"
      },
      "outputs": [],
      "source": [
        "# setting up weights directory and downloading a particular sam model\n",
        "!mkdir {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NoAXK80aHYBH"
      },
      "outputs": [],
      "source": [
        "def load_image(img_name, img_address, resized_image_size):\n",
        "  # load single image\n",
        "  image_bgr = cv2.imread(img_address)\n",
        "  original_image_size = image_bgr.shape[1], image_bgr.shape[0]\n",
        "\n",
        "  image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "  resized_image = cv2.resize(image_rgb, resized_image_size) # for inference in YOLO\n",
        "\n",
        "  return image_bgr, image_rgb, resized_image, original_image_size\n",
        "\n",
        "\n",
        "def draw_bboxes_xyxyn(bboxes, img):\n",
        "  colors = [(150, 150, 150)]\n",
        "  drawn_img = img.copy()\n",
        "  for i, box in enumerate(bboxes):\n",
        "    x, y, x1, y1 = box\n",
        "    x, x1 = x*img.shape[1], x1*img.shape[1]\n",
        "    y, y1 = y*img.shape[0], y1*img.shape[0]\n",
        "    cv2.rectangle(drawn_img, (int(x), int(y)), (int(x1), int(y1)), colors[0], 10)\n",
        "  return drawn_img\n",
        "\n",
        "\n",
        "def get_sam_masks(yolo_result, sam, resized_image):\n",
        "  # multiple bounding boxes as input for a single image\n",
        "  input_boxes = yolo_result.boxes.xyxy\n",
        "  class_ids = yolo_result.boxes.cls.cpu().numpy()\n",
        "\n",
        "  mask_predictor = SamPredictor(sam)\n",
        "  transformed_boxes = mask_predictor.transform.apply_boxes_torch(input_boxes, resized_image.shape[:2])\n",
        "  mask_predictor.set_image(resized_image)\n",
        "  masks, iou_predictions, low_res_masks = mask_predictor.predict_torch(\n",
        "      point_coords=None,\n",
        "      point_labels=None,\n",
        "      boxes=transformed_boxes,\n",
        "      multimask_output=False\n",
        "  )\n",
        "  return masks, class_ids\n",
        "\n",
        "def create_detections(masks, class_ids):\n",
        "  # creating Detections object for all the masks\n",
        "  xyxys = np.array([sv.mask_to_xyxy(masks=i.cpu()) for i in masks])\n",
        "  xyxys = xyxys.squeeze(1)\n",
        "  numpy_masks = masks.cpu().numpy().squeeze(1)\n",
        "  detections = sv.Detections(\n",
        "        class_id = class_ids,\n",
        "        xyxy=xyxys,\n",
        "        mask=numpy_masks\n",
        "  )\n",
        "  return detections\n",
        "\n",
        "def draw_masks_image(image_bgr, detections):\n",
        "  # bounding boxes and segmented areas\n",
        "  box_annotator = sv.BoxAnnotator(color=sv.Color.red(), thickness=10)\n",
        "  mask_annotator = sv.MaskAnnotator(color=sv.Color.red())\n",
        "  source_image = image_bgr.copy()\n",
        "  segmented_image = image_bgr.copy()\n",
        "\n",
        "  source_image = box_annotator.annotate(scene=source_image,\n",
        "                                        detections=detections,\n",
        "                                        skip_label=False)\n",
        "  segmented_image = mask_annotator.annotate(scene=segmented_image,\n",
        "                                            detections=detections)\n",
        "\n",
        "  # plot_grid = sv.plot_images_grid(\n",
        "  #       images=[source_image, segmented_image],\n",
        "  #       grid_size=(1, 2),\n",
        "  #       titles=['image with SAM BB', 'segmented image'],\n",
        "  #       size=(20, 20)\n",
        "  #   )\n",
        "  return segmented_image\n",
        "\n",
        "def create_detections(masks, class_ids):\n",
        "  # creating Detections object for all the masks\n",
        "  xyxys = np.array([sv.mask_to_xyxy(masks=i.cpu()) for i in masks])\n",
        "  xyxys = xyxys.squeeze(1)\n",
        "  numpy_masks = masks.cpu().numpy().squeeze(1)\n",
        "  detections = sv.Detections(\n",
        "        class_id = class_ids,\n",
        "        xyxy=xyxys,\n",
        "        mask=numpy_masks\n",
        "  )\n",
        "  return detections\n",
        "\n",
        "def remove_small_contours(masks):\n",
        "    torch_masks = []\n",
        "    for mask in masks:\n",
        "        single_mask = np.array(mask[0].cpu()).astype(np.uint8).copy()\n",
        "\n",
        "        contours, hierarchy = cv2.findContours(\n",
        "            single_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "        if len(contours)>1:\n",
        "            cntsSorted = sorted(contours, key=lambda x: cv2.contourArea(x))\n",
        "            cv2.drawContours(single_mask, cntsSorted[:-1], -1, color=0, thickness=cv2.FILLED)\n",
        "        torch_mask = torch.tensor(single_mask.astype(bool)).cuda()[None, None, :, :]\n",
        "        torch_masks.append(torch_mask)\n",
        "    torch_masks = torch.concat(torch_masks)\n",
        "    return torch_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk7qAu0yF3pi"
      },
      "source": [
        "### Establish Google Drive Connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Aa_m9dFxFb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5aZ_Z4OQ5fu"
      },
      "source": [
        "### Custom Variables - I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "yJGU3UgvQ4Gb"
      },
      "outputs": [],
      "source": [
        "yolo_model_address = '/content/drive/MyDrive/Projects/Coral Microfragmentation/coral_detection_model/Yolo + SAM/yolo_ar_best.pt' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_g1as8fF-Xc"
      },
      "source": [
        "### Setting up the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZaLhfW2vGeir"
      },
      "outputs": [],
      "source": [
        "# YOLO model\n",
        "model = YOLO(yolo_model_address)\n",
        "\n",
        "# SAM model\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_TYPE = \"vit_h\"\n",
        "\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHeReE3KQF6_"
      },
      "source": [
        "### Custom Variables - II"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "cellView": "form",
        "id": "ZuIGo2BMM5Af"
      },
      "outputs": [],
      "source": [
        "address_of_image_dir = \"/content/drive/MyDrive/Projects/Coral Microfragmentation/coral_detection_model/Coral Detection Test Images/23-24 Coral Table Test Images\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJsmdPowHjOS"
      },
      "source": [
        "### Getting the Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "2Ryr-kI1ITZH"
      },
      "outputs": [],
      "source": [
        "# Reading in multiple images\n",
        "img_addresses = []\n",
        "for extension in POSSIBLE_IMAGE_EXTENSIONS:\n",
        "  address_of_all_images = os.path.join(address_of_image_dir, f\"*.{extension}\")\n",
        "  current_img_addresses = [i for i in glob.iglob(address_of_all_images)]\n",
        "  img_addresses.extend(current_img_addresses)\n",
        "img_names = [i.split(\"/\")[-1] for i in img_addresses]\n",
        "\n",
        "image_bgrs = []\n",
        "image_rgbs = []\n",
        "resized_images = []\n",
        "original_image_sizes = []\n",
        "\n",
        "for j, i_img_address in enumerate(img_addresses):\n",
        "  img_name = img_names[j]\n",
        "  image_bgr, image_rgb, resized_image, original_image_size = load_image(img_name, i_img_address, RESIZED_IMAGE_SIZE)\n",
        "  image_bgrs.append(image_bgr)\n",
        "  image_rgbs.append(image_rgb)\n",
        "  resized_images.append(resized_image)\n",
        "  original_image_sizes.append(original_image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq41LpHlH4ik"
      },
      "outputs": [],
      "source": [
        "# Getting YOLO predictions for multiple images\n",
        "results = model.predict(resized_images, conf=0.1)\n",
        "images_with_bboxes = [draw_bboxes_xyxyn(result.boxes.xyxyn, image_rgb) for result in results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "qCtfEaIxJlTx"
      },
      "outputs": [],
      "source": [
        "# Getting SAM predictions for multiple images\n",
        "all_masks = []\n",
        "all_class_ids = []\n",
        "for j, result in enumerate(results):\n",
        "  if result.boxes.shape[0] != 0:\n",
        "    masks, class_ids = get_sam_masks(result, sam, resized_images[j])\n",
        "    ### TODO: Find better way to deal with multiple small contours\n",
        "    masks = remove_small_contours(masks)\n",
        "  else:\n",
        "    class_ids = np.array([], dtype=float)\n",
        "    masks = torch.zeros((0, 1, RESIZED_IMAGE_SIZE[1], RESIZED_IMAGE_SIZE[0]), dtype=bool)\n",
        "  all_masks.append(masks)\n",
        "  all_class_ids.append(class_ids)\n",
        "\n",
        "# Resizing segmentation maps for multiple images\n",
        "all_big_masks = []\n",
        "for j, masks in enumerate(all_masks):\n",
        "  big_masks = [torch.nn.functional.interpolate(i.to(torch.float32).unsqueeze(0),\n",
        "                                              size=(original_image_sizes[j][1], original_image_sizes[j][0])).to(bool)\n",
        "                                              for i in masks]\n",
        "  if big_masks:\n",
        "      big_masks = torch.stack(big_masks).squeeze(1)\n",
        "  all_big_masks.append(big_masks)\n",
        "\n",
        "# Creating detection objects for multiple images\n",
        "all_detections = []\n",
        "for j, big_masks in enumerate(all_big_masks):\n",
        "  all_detections.append(create_detections(big_masks, all_class_ids[j]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN2pi7SzKOCU"
      },
      "source": [
        "### Saving the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "pZC--ZDaKTJ4"
      },
      "outputs": [],
      "source": [
        "inference_dataset = sv.DetectionDataset([\"coral\", 'ref'],\n",
        "                                        {f\"{img_name}\": image_rgb for img_name, image_rgb in zip(img_names, image_rgbs)},\n",
        "                                        {f\"{img_name}\": detections for img_name, detections in zip(img_names, all_detections)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "NOnBC953KfCd"
      },
      "outputs": [],
      "source": [
        "save_coco_file_address = f\"{address_of_image_dir}/Output/coco_from_AI.json\"\n",
        "inference_dataset.as_coco(annotations_path=save_coco_file_address,\n",
        "                          approximation_percentage=0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1PI8L2EvE9F_",
        "pk7qAu0yF3pi",
        "s_g1as8fF-Xc",
        "yJsmdPowHjOS",
        "HN2pi7SzKOCU"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}